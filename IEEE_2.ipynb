{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpKfU7HV9kvtGBDTsRa000",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/07rudrajoshi-pixel/Rudra_Joshi-2025A2PS1480H-/blob/main/IEEE_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAbyhtytbXKy"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers faiss-cpu ranx rank-bm25 fastapi uvicorn datasets tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "mrJCupg3bdtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed=42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "dmcMv3qSbfIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"data/raw\", exist_ok=True)\n",
        "os.makedirs(\"data/processed\",exist_ok=True)"
      ],
      "metadata": {
        "id": "gM9JSRncbgjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOADING DATA"
      ],
      "metadata": {
        "id": "isxcsMi_bl51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "os.makedirs(\"data/raw\", exist_ok=True)\n",
        "\n",
        "dataset = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\")\n",
        "dataset = dataset.shuffle(seed=42).select(range(50_000))"
      ],
      "metadata": {
        "id": "0MCaYJivbi3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "8mJaIuMkbz5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:2]"
      ],
      "metadata": {
        "id": "2BZC8Dceb2Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset))"
      ],
      "metadata": {
        "id": "02WCYil2b3ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Each row has a format\")\n",
        "print(\"the format is \")\n",
        "print(\"   'passages':\")\n",
        "print(\"   'passage_text'\")\n",
        "print(\"   'url'\")\n",
        "print(\"   'query:'\")\n",
        "print(\"   'query_id':\")\n",
        "print(\"   'query_type'\")"
      ],
      "metadata": {
        "id": "dHBFMDhxb418"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.features"
      ],
      "metadata": {
        "id": "yPY_tSeib6TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "id": "cfra2J6vb7cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"choosing random 50K rows to work on \")\n",
        "dataset = dataset.shuffle(seed=42).select(range(50000))\n",
        "print(dataset.shape)\n",
        "print(\"50K rows choosen randomly\")"
      ],
      "metadata": {
        "id": "cR86r02Vb8z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "doc_id = 0\n",
        "\n",
        "for row in dataset:\n",
        "    for passage, is_rel in zip(\n",
        "        row[\"passages\"][\"passage_text\"],\n",
        "        row[\"passages\"][\"is_selected\"]\n",
        "    ):\n",
        "        documents.append({\n",
        "            \"doc_id\": f\"d{doc_id}\",\n",
        "            \"text\": passage,\n",
        "            \"is_relevant\": int(is_rel),\n",
        "            \"query\": row[\"query\"]\n",
        "        })\n",
        "        doc_id += 1\n",
        "\n",
        "with open(\"data/processed/documents.json\", \"w\") as f:\n",
        "    json.dump(documents, f, indent=2)"
      ],
      "metadata": {
        "id": "QoFSPVJJb96F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total documents: \",len(documents))"
      ],
      "metadata": {
        "id": "MYc11Z5Rb_tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/processed/documents.json\", \"w\") as f:\n",
        "    json.dump(documents, f, indent=2)"
      ],
      "metadata": {
        "id": "LUZyy33CcBSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"We actually need query, positive documents and negative documents\")\n",
        "print(\"positve documents have is_selected==1\")\n",
        "print(\"negative documents have is_selected==0\")"
      ],
      "metadata": {
        "id": "-VwdbB94cCY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating required triplet\")\n",
        "train_triplets=[]\n",
        "for row in tqdm(dataset):\n",
        "  query=row['query']\n",
        "  passages=row['passages']\n",
        "\n",
        "  positives=[\n",
        "      p for p, s in zip(passages[\"passage_text\"], passages[\"is_selected\"])\n",
        "       if s==1\n",
        "  ]\n",
        "\n",
        "  negatives=[\n",
        "      p for p, s in zip(passages[\"passage_text\"], passages[\"is_selected\"])\n",
        "       if s==0\n",
        "  ]\n",
        "  if positives and negatives:\n",
        "    train_triplets.append({\n",
        "        \"query\":query,\n",
        "        \"positive\":positives[0],\n",
        "        \"negative\":random.choice(negatives)\n",
        "    })"
      ],
      "metadata": {
        "id": "2jYqEhMocEEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Triplets generated\")\n",
        "print(f\"Training triplets: {len(train_triplets)}\")"
      ],
      "metadata": {
        "id": "doq52teNcKZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(train_triplets)\n",
        "\n",
        "split_idx = int(0.9 * len(train_triplets))\n",
        "\n",
        "train_triplets_split = train_triplets[:split_idx]\n",
        "val_triplets_split   = train_triplets[split_idx:]\n",
        "\n",
        "with open(\"data/processed/train_triplets.json\", \"w\") as f:\n",
        "    json.dump(train_triplets_split, f, indent=2)\n",
        "\n",
        "with open(\"data/processed/val_triplets.json\", \"w\") as f:\n",
        "    json.dump(val_triplets_split, f, indent=2)\n",
        "\n",
        "print(\"Train triplets:\", len(train_triplets_split))\n",
        "print(\"Val triplets:\", len(val_triplets_split))"
      ],
      "metadata": {
        "id": "kTCFc7sBcMHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/processed/train_triplet.json\", \"w\") as f:\n",
        "    json.dump(train_triplets, f, indent=2)\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "random.shuffle(train_triplets)\n",
        "\n",
        "\n",
        "split_idx = int(len(train_triplets) * 0.9)\n",
        "\n",
        "train_examples = []\n",
        "val_examples = []\n",
        "\n",
        "for i, triplet in enumerate(train_triplets):\n",
        "\n",
        "    example = [triplet[\"query\"], triplet[\"positive\"]]\n",
        "    if i < split_idx:\n",
        "        train_examples.append(example)\n",
        "    else:\n",
        "        val_examples.append(example)\n",
        "\n",
        "\n",
        "with open(\"data/processed/train_pairs.json\", \"w\") as f:\n",
        "    json.dump(train_examples, f, indent=2)\n",
        "\n",
        "\n",
        "with open(\"data/processed/val_pairs.json\", \"w\") as f:\n",
        "    json.dump(val_examples, f, indent=2)\n",
        "\n",
        "print(f\"Generated {len(train_examples)} training pairs and {len(val_examples)} validation pairs.\")"
      ],
      "metadata": {
        "id": "Uco5oL92cOTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_size=100\n",
        "test_dataset=dataset.shuffle(seed=42).select(range(test_size))"
      ],
      "metadata": {
        "id": "YLuEonRUcRni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries={}\n",
        "for i, row  in enumerate(test_dataset):\n",
        "  queries[str(i)]=row['query']"
      ],
      "metadata": {
        "id": "tJdAWjQ8cTcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "\n",
        "\n",
        "with open(\"data/processed/queries.json\", \"w\") as f:\n",
        "    json.dump(queries, f, indent=2)\n",
        "\n",
        "print(\"Queries saved to data/processed/queries.json\")"
      ],
      "metadata": {
        "id": "5TgGV76lcVgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f42bd3-4500-4635-94fb-643bd5644269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries saved to data/processed/queries.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading triplets\")\n",
        "def load_triplets(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    examples = []\n",
        "    for item in data:\n",
        "        examples.append(\n",
        "            InputExample(\n",
        "                texts=[\n",
        "                    item[\"query\"],\n",
        "                    item[\"positive\"],\n",
        "                    item[\"negative\"]\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "    return examples\n",
        "\n",
        "print(\"Triplets loaded\")"
      ],
      "metadata": {
        "id": "eUv1Xlu2cXUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e16cf4-52d6-4564-acb5-a968ee446c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading triplets\n",
            "Triplets loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = load_triplets(\"data/processed/train_triplets.json\")\n",
        "val_examples   = load_triplets(\"data/processed/val_triplets.json\")\n",
        "\n",
        "print(f\"Train triplets: {len(train_examples)}\")\n",
        "print(f\"Validation triplets: {len(val_examples)}\")"
      ],
      "metadata": {
        "id": "L3iKQ5ZgcaoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa39a7ec-657c-4864-fa7a-f6707276bad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train triplets: 43578\n",
            "Validation triplets: 4842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/processed/train_triplets.json\", \"w\") as f:\n",
        "    json.dump(train_triplets_split, f, indent=2)\n",
        "\n",
        "with open(\"data/processed/val_triplets.json\", \"w\") as f:\n",
        "    json.dump(val_triplets_split, f, indent=2)"
      ],
      "metadata": {
        "id": "wEfpfiyVcc0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "output_corpus_path = \"data/processed/corpus.json\"\n",
        "\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "\n",
        "with open(\"data/processed/documents.json\", \"r\") as f:\n",
        "    documents_list = json.load(f)\n",
        "\n",
        "corpus = {}\n",
        "for doc in documents_list:\n",
        "    corpus[doc[\"doc_id\"]] = doc[\"text\"]\n",
        "\n",
        "with open(output_corpus_path, \"w\") as f:\n",
        "    json.dump(corpus, f, indent=2)\n",
        "\n",
        "print(f\"Corpus saved with {len(corpus)} documents to {output_corpus_path}\")"
      ],
      "metadata": {
        "id": "qgmwgo-fceUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3765f8c9-5cc3-441b-b597-823aa4ef972c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus saved with 286033 documents to data/processed/corpus.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    return text.strip()\n",
        "print(\"Data cleaned\")"
      ],
      "metadata": {
        "id": "XX-8E-fRcltM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b62c4bc-0d6d-4de5-de93-c174d5d59f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL TRAINING"
      ],
      "metadata": {
        "id": "rRGusxD2cqQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Intialsing Encoder Model \")\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"Model device:\", device)"
      ],
      "metadata": {
        "id": "bAGrWg7rcnpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataloaders Creation\")\n",
        "train_dataloader = DataLoader(\n",
        "    train_examples,\n",
        "    shuffle=True,\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_examples,\n",
        "    shuffle=False,\n",
        "    batch_size=16\n",
        ")"
      ],
      "metadata": {
        "id": "HvzY7BKPco-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2541b1b9-7479-4c33-d6ec-86423a649338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders Creation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = losses.TripletLoss(\n",
        "    model=model,\n",
        "    distance_metric=losses.TripletDistanceMetric.COSINE,\n",
        "    triplet_margin=0.3\n",
        ")\n",
        "print(f\"Model Loaded on {model.device}\")\n",
        "print(\"Loss Function Initialised\")"
      ],
      "metadata": {
        "id": "zH1sD_dacvzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c99c79b-38dc-455a-dc1d-38f0d52cfe27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Loaded on cpu\n",
            "Loss Function Initialised\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINE TUNING"
      ],
      "metadata": {
        "id": "FKG3elircxlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2\n",
        "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n",
        "\n",
        "output_path = \"models/fine_tuned_retriever\"\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "print(\"Fine tuning started\")\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "SlnNnzdxcxFZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d77c265b-6f23-409b-ee17-eab9028cc171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine tuning started\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training the model\")\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=num_epochs,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path=output_path,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "print(\"Model Trained\")"
      ],
      "metadata": {
        "id": "SVp9r8nYc2J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(output_path)\n",
        "print(\"Fine-tuned model saved at:\", output_path)"
      ],
      "metadata": {
        "id": "bb4Gb9r_c33g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('gpu check')\n",
        "assert torch.cuda.is_available(), \"GPU is required for fine-tuning\"\n",
        "\n",
        "device = \"cuda\"\n",
        "print(\"Using GPU:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "fpTVHpX8c7wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_TRIPLETS_PATH = \"data/processed/train_triplets.json\"\n",
        "VAL_TRIPLETS_PATH = \"data/processed/val_triplets.json\"\n",
        "\n",
        "with open(TRAIN_TRIPLETS_PATH) as f:\n",
        "    train_triplets = json.load(f)\n",
        "\n",
        "with open(VAL_TRIPLETS_PATH) as f:\n",
        "    val_triplets = json.load(f)\n",
        "\n",
        "print(f\"Train triplets: {len(train_triplets)}\")\n",
        "print(f\"Validation triplets: {len(val_triplets)}\")"
      ],
      "metadata": {
        "id": "RCPUhxedc9gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = [\n",
        "    InputExample(\n",
        "        texts=[t[\"query\"], t[\"positive\"], t[\"negative\"]]\n",
        "    )\n",
        "    for t in train_triplets\n",
        "]\n",
        "print(\"Triplets converted to SentenceTransfprmer format\") # required for triplet loss"
      ],
      "metadata": {
        "id": "j2JxVq2rc_fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_examples,\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    drop_last=True\n",
        ")"
      ],
      "metadata": {
        "id": "-6CKsJexdBad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\n",
        "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"Model loaded on:\", model.device)\n",
        "print(\"Model loaded\")"
      ],
      "metadata": {
        "id": "nTVkdW4XdCxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = losses.TripletLoss(\n",
        "    model=model,\n",
        "    distance_metric=losses.TripletDistanceMetric.COSINE,\n",
        "    triplet_margin=0.3\n",
        ")\n",
        "print(\"Triplet loss defined\")"
      ],
      "metadata": {
        "id": "-Q2bi7iqdEy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f6f2b52-8f9c-4129-f969-7faa7511b36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triplet loss defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 2\n",
        "\n",
        "warmup_steps = int(\n",
        "    len(train_dataloader) * NUM_EPOCHS * 0.1\n",
        ")\n",
        "\n",
        "OUTPUT_PATH = \"models/fine_tuned_dense_retriever\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)"
      ],
      "metadata": {
        "id": "d8YQBzsGdGNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting fine-tuning\")\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=NUM_EPOCHS,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path=OUTPUT_PATH,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "print(\"Fine-tuning completed\")"
      ],
      "metadata": {
        "id": "dckGBaiwdH3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(OUTPUT_PATH)\n",
        "print(f\"Model saved to {OUTPUT_PATH}\")"
      ],
      "metadata": {
        "id": "Vbo6D4aXdJzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAISS RETRIEVAL"
      ],
      "metadata": {
        "id": "aoA_hbkhdM6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "AK7_2t3SdLKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading Model\")\n",
        "MODEL_PATH = \"models/fine_tuned_dense_retriever\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = SentenceTransformer(MODEL_PATH, device=device)\n",
        "model.eval()#disables dropouts"
      ],
      "metadata": {
        "id": "rEXtXYaldQ79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fine tuned model loaded\")"
      ],
      "metadata": {
        "id": "CDVZ6qSEdSSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "os.makedirs(\"data/embeddings\", exist_ok=True)\n",
        "\n",
        "\n",
        "with open(\"data/processed/corpus.json\", \"r\") as f:\n",
        "    corpus = json.load(f)\n",
        "\n",
        "doc_ids = list(corpus.keys())\n",
        "corpus_texts = [corpus[doc_id] for doc_id in doc_ids]\n",
        "\n",
        "\n",
        "embeddings_path = \"data/embeddings/corpus_embeddings.npy\"\n",
        "if not os.path.exists(embeddings_path):\n",
        "    print(\"Generating corpus embeddings...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        corpus_embeddings = model.encode(corpus_texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "    np.save(embeddings_path, corpus_embeddings)\n",
        "    print(f\"Corpus embeddings saved to {embeddings_path}\")\n",
        "else:\n",
        "    print(f\"Corpus embeddings already exist at {embeddings_path}. Loading them.\")\n",
        "\n",
        "\n",
        "corpus_embeddings = np.load(embeddings_path)\n",
        "\n",
        "print(\"loaded corpus and embeddings\")"
      ],
      "metadata": {
        "id": "CheOVtQHdUHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Normalising Embedding -needed for cosine similarity \")\n",
        "faiss.normalize_L2(corpus_embeddings)\n",
        "\n",
        "embedding_dim = corpus_embeddings.shape[1]\n",
        "print(\"Embedding dimension:\", embedding_dim)"
      ],
      "metadata": {
        "id": "3oNkTH-adwIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.IndexFlatIP(embedding_dim)\n",
        "index.add(corpus_embeddings)\n",
        "\n",
        "print(\"FAISS index built\")\n",
        "print(\"Number of documents indexed:\", index.ntotal)"
      ],
      "metadata": {
        "id": "RB2PyVbkdzXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/processed/queries.json\", \"r\") as f:\n",
        "    queries = json.load(f)\n",
        "\n",
        "query_ids = list(queries.keys())\n",
        "query_texts = [queries[qid] for qid in query_ids]"
      ],
      "metadata": {
        "id": "-34xhD4Hd3zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.makedirs(\"faiss_index\", exist_ok=True)\n",
        "\n",
        "embeddings = np.load(\"data/embeddings/corpus_embeddings.npy\")\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n",
        "\n",
        "dim = embeddings.shape[1]\n",
        "\n",
        "\n",
        "index = faiss.IndexFlatIP(dim)\n",
        "\n",
        "\n",
        "index.add(embeddings)\n",
        "\n",
        "print(\"Total vectors indexed:\", index.ntotal)\n",
        "\n",
        "\n",
        "faiss.write_index(index, \"faiss_index/index.faiss\")\n",
        "print(\"FAISS index saved to faiss_index/index.faiss\")"
      ],
      "metadata": {
        "id": "uSJWUwZ0CvtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_queries(queries, batch_size=32):\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(0, len(queries), batch_size):\n",
        "        batch = queries[i:i + batch_size]\n",
        "        with torch.no_grad():\n",
        "            emb = model.encode(\n",
        "                batch,\n",
        "                convert_to_numpy=True,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "        embeddings.append(emb)\n",
        "\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "query_embeddings = encode_queries(query_texts)\n",
        "print(\"Query embeddings generated\")"
      ],
      "metadata": {
        "id": "uEPxFsjKd5Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_K = 10\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "scores, indices = index.search(query_embeddings, TOP_K)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"FAISS search completed in {end_time - start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "fytLwAwAd64A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_results = {}\n",
        "\n",
        "for q_idx, qid in enumerate(query_ids):\n",
        "    retrieved_docs = []\n",
        "\n",
        "    for rank, doc_idx in enumerate(indices[q_idx]):\n",
        "        doc_id = doc_ids[doc_idx]\n",
        "        score = float(scores[q_idx][rank])\n",
        "\n",
        "        retrieved_docs.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"score\": score,\n",
        "            \"rank\": rank + 1\n",
        "        })\n",
        "\n",
        "    retrieval_results[qid] = retrieved_docs"
      ],
      "metadata": {
        "id": "-DPcj7AAd8kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "with open(\"results/faiss_results.json\", \"w\") as f:\n",
        "    json.dump(retrieval_results, f, indent=2)\n",
        "\n",
        "print(\"FAISS retrieval results saved\")"
      ],
      "metadata": {
        "id": "8mvN3xu4d-h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION METRICS"
      ],
      "metadata": {
        "id": "vRUETncpqxIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading corpus\n",
        "with open(\"data/processed/corpus.json\") as f:\n",
        "    corpus = json.load(f)\n",
        "\n",
        "doc_ids = list(corpus.keys())\n",
        "\n",
        "# Loading test queries\n",
        "queries_file_path = \"data/processed/queries.json\"\n",
        "with open(queries_file_path) as f:\n",
        "    queries = json.load(f)\n",
        "\n",
        "with open(\"data/processed/documents.json\", \"r\") as f:\n",
        "    all_documents = json.load(f)\n",
        "\n",
        "qrels = {}\n",
        "\n",
        "query_text_to_id = {text: qid for qid, text in queries.items()}\n",
        "\n",
        "for doc in all_documents:\n",
        "    if doc[\"is_relevant\"] == 1:\n",
        "        query_text = doc[\"query\"]\n",
        "\n",
        "        if query_text in query_text_to_id:\n",
        "            query_id = query_text_to_id[query_text]\n",
        "            if query_id not in qrels:\n",
        "                qrels[query_id] = {}\n",
        "            qrels[query_id][doc[\"doc_id\"]] = 1\n",
        "\n",
        "\n",
        "\n",
        "with open(\"data/processed/qrels.json\", \"w\") as f:\n",
        "    json.dump(qrels, f, indent=2)\n",
        "\n",
        "print(f\"Corpus size: {len(corpus)}\")\n",
        "print(f\"Test queries: {len(queries)}\")\n",
        "print(f\"QRELs loaded for {len(qrels)} queries\")"
      ],
      "metadata": {
        "id": "tjLOi6-7eAbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.read_index(\"faiss_index/index.faiss\")\n",
        "print(\"FAISS index loaded\")\n",
        "print(\"Indexed vectors:\", index.ntotal)"
      ],
      "metadata": {
        "id": "AVAie-3rq8r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\n",
        "    \"models/fine_tuned_dense_retriever\", # Corrected model path\n",
        "    device=\"cuda\"\n",
        ")"
      ],
      "metadata": {
        "id": "wblbeh61rk9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_texts = [queries[qid] for qid in queries]\n",
        "\n",
        "query_embeddings = model.encode(\n",
        "    query_texts,\n",
        "    batch_size=32,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True,\n",
        "    show_progress_bar=True\n",
        ")"
      ],
      "metadata": {
        "id": "yMSomKC9rldV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_K = 10\n",
        "\n",
        "distances, indices = index.search(query_embeddings, TOP_K)\n",
        "\n",
        "retrieved_docs = {}\n",
        "\n",
        "for i, qid in enumerate(queries.keys()):\n",
        "    retrieved_docs[qid] = [doc_ids[idx] for idx in indices[i]]"
      ],
      "metadata": {
        "id": "ON7s80yxrn4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(retrieved, relevant, k):\n",
        "    retrieved_k = retrieved[:k]\n",
        "    rel_set = set(relevant)\n",
        "    return len(set(retrieved_k) & rel_set) / k\n",
        "\n",
        "def recall_at_k(retrieved, relevant, k):\n",
        "    retrieved_k = retrieved[:k]\n",
        "    rel_set = set(relevant)\n",
        "    return len(set(retrieved_k) & rel_set) / len(rel_set) if rel_set else 0\n",
        "\n",
        "def reciprocal_rank(retrieved, relevant):\n",
        "    rel_set = set(relevant)\n",
        "    for i, doc_id in enumerate(retrieved):\n",
        "        if doc_id in rel_set:\n",
        "            return 1 / (i + 1)\n",
        "    return 0\n",
        "\n",
        "def average_precision(retrieved, relevant):\n",
        "    rel_set = set(relevant)\n",
        "    score = 0.0\n",
        "    hits = 0\n",
        "\n",
        "    for i, doc_id in enumerate(retrieved):\n",
        "        if doc_id in rel_set:\n",
        "            hits += 1\n",
        "            score += hits / (i + 1)\n",
        "\n",
        "    return score / len(rel_set) if rel_set else 0\n",
        "\n",
        "def ndcg_at_k(retrieved, relevant, k):\n",
        "    dcg = 0.0\n",
        "    for i, doc_id in enumerate(retrieved[:k]):\n",
        "        if doc_id in relevant:\n",
        "            dcg += 1 / np.log2(i + 2)\n",
        "\n",
        "    idcg = sum(1 / np.log2(i + 2) for i in range(min(len(relevant), k)))\n",
        "    return dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "def hit_at_k(retrieved, relevant, k):\n",
        "    return int(any(doc in relevant for doc in retrieved[:k]))"
      ],
      "metadata": {
        "id": "50K2ubcjrp9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "metrics = defaultdict(list)\n",
        "\n",
        "for qid in tqdm(queries.keys()):\n",
        "\n",
        "    retrieved_for_query = retrieval_results[qid]\n",
        "    retrieved_doc_ids = [item['doc_id'] for item in retrieved_for_query] # This will be a list of strings\n",
        "\n",
        "    relevant = qrels.get(qid, {}).keys()\n",
        "\n",
        "    metrics[\"P@5\"].append(precision_at_k(retrieved_doc_ids, relevant, 5))\n",
        "    metrics[\"R@5\"].append(recall_at_k(retrieved_doc_ids, relevant, 5))\n",
        "    metrics[\"MRR\"].append(reciprocal_rank(retrieved_doc_ids, relevant))\n",
        "    metrics[\"MAP\"].append(average_precision(retrieved_doc_ids, relevant))\n",
        "    metrics[\"nDCG@10\"].append(ndcg_at_k(retrieved_doc_ids, relevant, 10))\n",
        "    metrics[\"Hit@10\"].append(hit_at_k(retrieved_doc_ids, relevant, 10))"
      ],
      "metadata": {
        "id": "DuNSHnLzr3Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FINAL RESULTS\")\n",
        "\n",
        "for metric, values in metrics.items():\n",
        "    print(f\"{metric}: {np.mean(values):.4f}\")"
      ],
      "metadata": {
        "id": "OHrOhV6nr7q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXr_82f-C_73"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}